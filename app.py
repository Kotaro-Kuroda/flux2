"""
FLUX.2 Gradio App
ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ã€FLUX.2ã§ç”»åƒç”Ÿæˆã‚’è¡Œã†Webã‚¢ãƒ—ãƒª
"""

import gc

import gradio as gr
import torch
from diffusers import Flux2Pipeline
from PIL import Image


class FLUX2App:
    def __init__(self):
        self.pipe = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
        self.repo_id = "diffusers/FLUX.2-dev-bnb-4bit"

    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’åˆå›ã®ã¿ãƒ­ãƒ¼ãƒ‰"""
        if self.pipe is None:
            print("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.pipe = Flux2Pipeline.from_pretrained(
                self.repo_id,
                torch_dtype=self.torch_dtype
            ).to(self.device)

            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            self.pipe.enable_model_cpu_offload()
            print("ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        return self.pipe

    def generate_image(
        self,
        prompt: str,
        input_image: Image.Image,
        num_steps: int = 28,
        guidance_scale: float = 4.0,
        seed: int = 42,
        width: int = 1024,
        height: int = 768,
        progress=gr.Progress()
    ):
        """
        ç”»åƒã‚’ç”Ÿæˆ

        Args:
            prompt: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            input_image: å…¥åŠ›ç”»åƒï¼ˆä»»æ„ï¼‰
            num_steps: ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            width: å‡ºåŠ›ç”»åƒã®å¹…
            height: å‡ºåŠ›ç”»åƒã®é«˜ã•
            progress: Gradio Progress tracker

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸç”»åƒã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒã‚§ãƒƒã‚¯
            if not prompt or prompt.strip() == "":
                error_msg = "âŒ ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
                progress(0, desc=error_msg)
                return None, error_msg

            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            progress(0.1, desc="ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            pipe = self.load_model()

            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            progress(0.2, desc="âš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šä¸­...")
            generator = torch.Generator(device=self.device).manual_seed(seed)

            # å…¥åŠ›ç”»åƒã®å‡¦ç†
            images_input = [input_image] if input_image is not None else None

            # ç”»åƒç”Ÿæˆ
            progress(0.3, desc=f"ğŸ¨ ç”»åƒç”Ÿæˆä¸­... (0/{num_steps} steps)")
            print(f"ç”Ÿæˆé–‹å§‹: prompt='{prompt[:50]}...', steps={num_steps}, guidance={guidance_scale}")

            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’æ›´æ–°
            def callback(pipe, step_index, timestep, callback_kwargs):
                progress_value = 0.3 + (0.6 * (step_index + 1) / num_steps)
                progress(progress_value, desc=f"ğŸ¨ ç”»åƒç”Ÿæˆä¸­... ({step_index + 1}/{num_steps} steps)")
                return callback_kwargs

            result = pipe(
                prompt=prompt,
                image=images_input,
                generator=generator,
                num_inference_steps=num_steps,
                guidance_scale=guidance_scale,
                width=width,
                height=height,
                callback_on_step_end=callback,
            )

            output_image = result.images[0]

            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            progress(0.95, desc="ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ä¸­...")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            success_msg = f"âœ… ç”Ÿæˆå®Œäº†ï¼ (steps={num_steps}, guidance={guidance_scale}, seed={seed})"
            progress(1.0, desc=success_msg)
            return output_image, success_msg

        except torch.cuda.OutOfMemoryError as e:
            error_msg = f"âŒ VRAMä¸è¶³ã‚¨ãƒ©ãƒ¼: ãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šã¾ã›ã‚“ã€‚ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚„è§£åƒåº¦ã‚’ä¸‹ã’ã¦ãã ã•ã„ã€‚\nè©³ç´°: {str(e)}"
            print(error_msg)
            progress(0, desc="âŒ VRAMä¸è¶³ã‚¨ãƒ©ãƒ¼")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            return None, error_msg
        except Exception as e:
            error_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}\nè©³ç´°: {str(e)}"
            print(error_msg)
            progress(0, desc=f"âŒ {type(e).__name__}")
            return None, error_msg


def create_ui():
    """Gradio UIã‚’ä½œæˆ"""
    app = FLUX2App()

    with gr.Blocks(title="FLUX.2 ç”»åƒç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ¨ FLUX.2 ç”»åƒç”Ÿæˆã‚¢ãƒ—ãƒª

            ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å…¥åŠ›ç”»åƒï¼ˆä»»æ„ï¼‰ã‹ã‚‰ã€FLUX.2ã§æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                # å…¥åŠ›ã‚¨ãƒªã‚¢
                gr.Markdown("### å…¥åŠ›")

                prompt_input = gr.Textbox(
                    label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                    placeholder="ç”Ÿæˆã—ãŸã„ç”»åƒã®èª¬æ˜ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: a beautiful sunset over the oceanï¼‰",
                    lines=3,
                    value="a beautiful sunset over the ocean with vibrant colors"
                )

                image_input = gr.Image(
                    label="å…¥åŠ›ç”»åƒï¼ˆä»»æ„ï¼‰",
                    type="pil",
                    sources=["upload", "clipboard"]
                )

                with gr.Accordion("è©³ç´°è¨­å®š", open=False):
                    num_steps = gr.Slider(
                        minimum=10,
                        maximum=50,
                        value=28,
                        step=1,
                        label="ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆå¤šã„ã»ã©é«˜å“è³ªã ãŒæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰"
                    )

                    guidance_scale = gr.Slider(
                        minimum=1.0,
                        maximum=10.0,
                        value=4.0,
                        step=0.5,
                        label="ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆé«˜ã„ã»ã©ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿ å®Ÿï¼‰"
                    )

                    seed = gr.Number(
                        label="ã‚·ãƒ¼ãƒ‰å€¤ï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰",
                        value=42,
                        precision=0
                    )

                    with gr.Row():
                        width = gr.Slider(
                            minimum=512,
                            maximum=2048,
                            value=1024,
                            step=64,
                            label="å¹…"
                        )
                        height = gr.Slider(
                            minimum=512,
                            maximum=2048,
                            value=768,
                            step=64,
                            label="é«˜ã•"
                        )

                generate_btn = gr.Button("ğŸ¨ ç”Ÿæˆ", variant="primary", size="lg")

            with gr.Column(scale=1):
                # å‡ºåŠ›ã‚¨ãƒªã‚¢
                gr.Markdown("### å‡ºåŠ›")

                output_image = gr.Image(
                    label="ç”Ÿæˆç”»åƒ",
                    type="pil"
                )

                status_text = gr.Textbox(
                    label="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                    interactive=False
                )

        # ã‚µãƒ³ãƒ—ãƒ«ä¾‹
        gr.Markdown("### ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹")
        gr.Examples(
            examples=[
                ["a photo of a forest with mist swirling around the tree trunks"],
                ["a clean monochrome CAD-style technical line drawing"],
                ["a beautiful landscape with mountains and a lake at sunset"],
                ["an astronaut riding a horse on the moon"],
                ["a cute cat wearing sunglasses, digital art"],
            ],
            inputs=[prompt_input],
            label="ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚»ãƒƒãƒˆ"
        )

        # ã‚¤ãƒ™ãƒ³ãƒˆè¨­å®š
        generate_btn.click(
            fn=app.generate_image,
            inputs=[
                prompt_input,
                image_input,
                num_steps,
                guidance_scale,
                seed,
                width,
                height,
            ],
            outputs=[output_image, status_text]
        )

        gr.Markdown(
            """
            ---
            **ä½¿ã„æ–¹:**
            1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ï¼ˆå¿…é ˆï¼‰
            2. å…¥åŠ›ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ã€ç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
            3. è©³ç´°è¨­å®šã‚’èª¿æ•´ï¼ˆä»»æ„ï¼‰
            4. ã€Œç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯

            **æ³¨æ„:**
            - åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™
            - VRAMä¸è¶³ã®å ´åˆã¯ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚„è§£åƒåº¦ã‚’ä¸‹ã’ã¦ãã ã•ã„
            """
        )

    return demo


if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
